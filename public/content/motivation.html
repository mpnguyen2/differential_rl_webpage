<h2>Motivation</h2>
<ul>
  <li><b>Reinforcement Learning (RL)</b> made strong progress in robotics, biology, and language.
      In <b>scientific applications with limited data</b>, RL struggles with:
    <ol>
      <li>low sample efficiency,</li>
      <li>weak alignment with physical laws, and</li>
      <li>limited theoretical guarantees.</li>
    </ol>
  </li>
  <li>Even with reward shaping, <b>Model-free RL</b> lacks built-in physics priors and remains sample-inefficient.</li>
  <li><b>Model-based RL</b> can be efficient but often requires exact reward functionals/gradients or trajectory restarts.</li>
  <li>New approach: RL → <b>continuous-time control formulation</b> → Hamiltonian <b>differential dual</b> → algorithm
      ⇒ <b>physics priors</b>, sample-efficient updates, and <b>pointwise</b> learning.</li>
</ul>

<h2 id="physics-intuition">Physics Intuition: From Newton to Hamiltonian Dual Control</h2>
<p>
  Newtonian dynamics uses forces <span class="math">\(F=m\ddot{s}\)</span>. Lagrangian mechanics
  rephrases motion via energies and stationary action; the physical path makes the action stationary.
</p>
<div class="code">
  \(\displaystyle
  \mathcal{S}[s]=\int_{0}^{T}\mathcal{L}(s,\dot{s},t)\,dt,\qquad
  \frac{\partial \mathcal{L}}{\partial s}=\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{s}}
  \ \text{(Euler–Lagrange)}.
  \) <br/>
  For \(\mathcal{L}=\tfrac{1}{2}m\|\dot{s}\|^2-\mathcal{V}(s)\): \(m\ddot{s}=-\nabla \mathcal{V}(s)\).
</div>
<p>
  Viewing <span class="math">\(a=\dot{s}\)</span> casts this as control. The HJB equation links value gradients to a
  Hamiltonian; with adjoint <span class="math">\(p=\partial V/\partial s\)</span> we recover a dual, symplectic flow—
  the bridge we use for continuous-time RL.
</p>
<div class="code">
  \(\displaystyle
  \frac{\partial V}{\partial t}+\max_{a}\!\Big(\tfrac{\partial V}{\partial s}f(s,a)-\mathcal{L}(s,a)\Big)=0,
  \quad
  \mathcal{H}(s,p,a)=p^{\top}f(s,a)-\mathcal{L}(s,a),\ p=\tfrac{\partial V}{\partial s}.
  \)
</div>

<h2 id="td-error">Revisit the TD Error</h2>
<div class="code">
  For \(s'=s+\Delta_t f(s,a)\) (take \(\Delta_t=1\)): <br/>
  \(\displaystyle
  \underbrace{r(s,a)+V(s')-V(s)}_{\text{TD error}}
  \approx r(s,a)+f(s,a)\,\partial_s V(s)
  =-\ \mathcal{H}\!\left(s,\,-\partial_s V(s),\,a\right).
  \)
</div>
<ul>
  <li>The local critic signal equals (minus) the Hamiltonian at the value gradient.</li>
  <li>As \(\Delta_t\to 0\), this approaches the continuous-time \(q\) quantity.</li>
  <li>Motivates <i>local</i>, physics-aligned targets for learning.</li>
</ul>
