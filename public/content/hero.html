<h2>Overview</h2>
<p><b>Title:</b> A Differential and Pointwise Control Approach to Reinforcement Learning</p>
<p><b>Authors:</b> Minh Nguyen &amp; Chandrajit Bajaj &nbsp;&nbsp; <b>Affiliation:</b> University of Texas at Austin</p>
<p>Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor
    sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning
    (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a
    differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures
    consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop
    Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators
    along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence
    guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of
    $\mathcal{O}(K^{5/6})$. Empirically, dfPO outperforms standard RL baselines on representative scientific computing
    tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained
    conditions.</p>